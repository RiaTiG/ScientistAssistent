import os
import tempfile
import torch
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, BotCommand
from telegram.ext import Application, CommandHandler, MessageHandler, CallbackQueryHandler, ContextTypes, filters
from sentence_transformers import SentenceTransformer, util
import PyPDF2
from docx import Document
import nltk
from nltk.tokenize import sent_tokenize
import logging
import traceback
import requests
import json
from datetime import datetime
import telegram
import re
import hashlib
import time
from config import yandexAPI, folder_id, telegramAPI

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')

def extract_text_from_pdf(pdf_file):
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞: {pdf_file}")
        text = ""
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        total_pages = len(pdf_reader.pages)
        logger.info(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ PDF: {total_pages}")
        
        for i, page in enumerate(pdf_reader.pages, 1):
            try:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {i} –∏–∑ {total_pages}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {i}: {str(e)}")
                continue
        
        if not text.strip():
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞")
            return None
            
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç –∏–∑ PDF. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        return text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def text_from_docx(docx_file):
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞: {docx_file}")
        doc = Document(docx_file)
        text = ""
        
        for i, paragraph in enumerate(doc.paragraphs, 1):
            if paragraph.text.strip():
                text += paragraph.text + "\n"
            if i % 100 == 0:
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
        
        if not text.strip():
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ DOCX —Ñ–∞–π–ª–∞")
            return None
            
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç –∏–∑ DOCX. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        return text
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOCX: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def process_document(file_path):
    try:
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: {file_path}")
        
        if not os.path.exists(file_path):
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return None
            
        if file_path.endswith('.pdf'):
            return extract_text_from_pdf(file_path)
        elif file_path.endswith(('.doc', '.docx')):
            return text_from_docx(file_path)
        else:
            logger.error(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")
            return None
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def clean_pdf(text):
    text = ' '.join(text.split())
    text = re.sub(r'\.+', '.', text)
    text = re.sub(r'\s+[–∞-—è–ê-–Øa-zA-Z0-9]\s+', ' ', text)
    text = re.sub(r'[^–∞-—è–ê-–Øa-zA-Z0-9\s.,!?;:()\-]', '', text)
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def valid_sentence(sentence):

    if len(sentence) < 10:
        return False
    if not any(c.isalpha() for c in sentence):
        return False
    if sentence.count(',') > 5 or sentence.count(';') > 3:
        return False
    if sentence.isupper() and len(sentence) < 50:
        return False
    if re.search(r'[A-Za-z]+\s*=\s*[A-Za-z0-9]+', sentence):
        return True 
    if re.search(r'\b[A-Z]{2,}\b', sentence):
        return True 
    return True

def calculate_importance(sentence, position, total_sentences):

    importance = 0.0
    # –ü–æ–∑–∏—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if position < total_sentences * 0.2 or position > total_sentences * 0.8:
        importance += 0.3  
    # –î–ª–∏–Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    length = len(sentence.split())
    if 10 <= length <= 40:
        importance += 0.2
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    key_terms = [
        'however', 'therefore', 'conclude', 'result', 'important', 'significant',
        'study', 'research', 'analysis', 'method', 'approach', 'findings',
        'demonstrate', 'show', 'prove', 'establish', 'identify', 'determine',
        'investigate', 'examine', 'analyze', 'evaluate', 'assess', 'consider',
        'suggest', 'indicate', 'reveal', 'observe', 'note', 'find',
        'conclusion', 'summary', 'overview', 'introduction', 'background',
        'objective', 'aim', 'goal', 'purpose', 'main', 'primary', 'key'
    ]

    for term in key_terms:
        if term.lower() in sentence.lower():
            importance += 0.15
    if re.search(r'[A-Za-z]+\s*=\s*[A-Za-z0-9]+', sentence):
        importance += 0.25
    if re.search(r'\d+%|\d+\.\d+|\d+\s*/\s*\d+', sentence):
        importance += 0.2
    if re.search(r'is\s+defined|means|refers\s+to|consists\s+of', sentence.lower()):
        importance += 0.2
    return importance

def summarize_pdf(text, max_length=250, min_length=50):
    try:
        text = clean_pdf(text)
        sentences = []
        current_sentence = []

        for line in text.split('.'):
            line = line.strip()
            if not line:
                continue
            
            if len(line) > 3 and not line.isupper() and not any(c.isdigit() for c in line):
                current_sentence.append(line)
                
                if len(' '.join(current_sentence)) > 15:
                    full_sentence = ' '.join(current_sentence)
                    if valid_sentence(full_sentence):
                        sentences.append(full_sentence)
                    current_sentence = []
        
        if current_sentence:
            full_sentence = ' '.join(current_sentence)
            if valid_sentence(full_sentence):
                sentences.append(full_sentence)
        
        if len(sentences) <= 3:
            return text
    
        embeddings = model.encode(sentences, convert_to_tensor=True)
        similarity_matrix = util.cos_sim(embeddings, embeddings)
        
        sentence_scores = []
        for i in range(len(sentences)):
            similar_scores = similarity_matrix[i][similarity_matrix[i] > 0.4]
            if len(similar_scores) > 0:
                base_score = torch.mean(similar_scores)
            else:
                base_score = torch.mean(similarity_matrix[i])
            
            importance_score = calculate_importance(sentences[i], i, len(sentences))
            final_score = float(base_score) + importance_score
            sentence_scores.append((final_score, i))
        sentence_scores.sort(reverse=True)
        num_sentences = max(min_length // 8, min(len(sentences), max_length // 8))
        selected_indices = sorted([idx for _, idx in sentence_scores[:num_sentences]])
        summary_text = ' '.join([sentences[i] for i in selected_indices])
        summary_text = clean_pdf(summary_text)
        summary = f"–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞:\n\n{summary_text}"
        
        return summary
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏ PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return text

def summarize_docx(text, max_length=130, min_length=30):
    try:
        sentences = sent_tokenize(text)
        valid_sentences = [s for s in sentences if valid_sentence(s)]
        
        if len(valid_sentences) <= 3:
            return text
            
        embeddings = model.encode(valid_sentences, convert_to_tensor=True)
        similarity_matrix = util.cos_sim(embeddings, embeddings)
        sentence_scores = []
        for i in range(len(valid_sentences)):
            score = torch.mean(similarity_matrix[i])
            importance = calculate_importance(valid_sentences[i], i, len(valid_sentences))
            final_score = float(score) + importance
            sentence_scores.append((final_score, i))
        
        sentence_scores.sort(reverse=True)
        num_sentences = max(min_length // 10, min(len(valid_sentences), max_length // 10))
        selected_indices = sorted([idx for _, idx in sentence_scores[:num_sentences]])
        summary_text = ' '.join([valid_sentences[i] for i in selected_indices])
        summary = f"–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞:\n\n{summary_text}"
        
        return summary
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏ DOCX: {str(e)}")
        logger.error(traceback.format_exc())
        return text

def search(query, year_from=None, year_to=None, limit=10):
    max_retries = 3  
    timeout = 30     
    
    for attempt in range(max_retries):
        try:
            logger.info(f"–ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ arXiv (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries}): query={query}, year_from={year_from}, year_to={year_to}")
            base_url = "http://export.arxiv.org/api/query"
            
            params = {
                "search_query": f"all:{query}",
                "start": 0,
                "max_results": limit * 2,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è —É—á–µ—Ç–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                "sortBy": "submittedDate",
                "sortOrder": "descending"
            }
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
            
            response = requests.get(
                base_url, 
                params=params, 
                headers=headers, 
                timeout=timeout
            )
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
            from xml.etree import ElementTree as ET
            root = ET.fromstring(response.content)
            
            ns = {'atom': 'http://www.w3.org/2005/Atom',
                  'arxiv': 'http://arxiv.org/schemas/atom'}
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏
            entries = root.findall('.//atom:entry', ns)
            if not entries:
                logger.warning("–°—Ç–∞—Ç—å–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –æ—Ç–≤–µ—Ç–µ API arXiv")
                return None  
            articles = []
            for entry in entries:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    title = entry.find('atom:title', ns).text.strip()
                    summary = entry.find('atom:summary', ns).text.strip()
                    published = entry.find('atom:published', ns).text
                    link = entry.find('atom:link[@title="pdf"]', ns).get('href')
                    year = int(published[:4])
                    if year_from and year < year_from:
                        continue
                    if year_to and year > year_to:
                        continue

                    authors = []
                    for author in entry.findall('atom:author/atom:name', ns):
                        authors.append(author.text)

                    categories = []
                    for category in entry.findall('atom:category', ns):
                        categories.append(category.get('term'))
                    
                    article = {
                        "title": title,
                        "authors": ", ".join(authors) or "–ê–≤—Ç–æ—Ä—ã –Ω–µ —É–∫–∞–∑–∞–Ω—ã",
                        "abstract": summary,
                        "url": link,
                        "year": str(year),
                        "categories": ", ".join(categories),
                        "venue": "arXiv",
                        "source": "arXiv",
                        "published_date": published
                    }
                    articles.append(article)
                    
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ arXiv: {str(e)}")
                    continue
                
            if not articles:
                logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π arXiv")
                return None
                
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            articles.sort(key=lambda x: x['published_date'], reverse=True)
            articles = articles[:limit]
                
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –∏–∑ arXiv")
            return articles
            
        except requests.exceptions.Timeout:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1}/{max_retries}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è —Å –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(wait_time)
            else:
                logger.error("–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫")
                return None
                
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –∫ API arXiv: {str(e)}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                time.sleep(wait_time)
            else:
                return None
                
        except ET.ParseError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ XML –æ—Ç–≤–µ—Ç–∞ arXiv: {str(e)}")
            return None
            
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π –Ω–∞ arXiv: {str(e)}")
            logger.error(traceback.format_exc())
            return None

def format_article(article):
    return (
        f"üìö {article['title']}\n\n"
        f"üë• –ê–≤—Ç–æ—Ä—ã: {article['authors']}\n"
        f"üìÖ –ì–æ–¥: {article['year']}\n"
        f"üè¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {article.get('categories', '–ù–µ —É–∫–∞–∑–∞–Ω—ã')}\n"
        f"üìù –ò—Å—Ç–æ—á–Ω–∏–∫: arXiv\n"
        f"\nüìù –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è:\n{article['abstract']}\n\n"
        f"üîó –°—Å—ã–ª–∫–∞: {article['url']}\n"
        f"{'‚îÄ' * 50}\n"
    )

def create_menu():
    keyboard = [
        [InlineKeyboardButton("üîç –ü–æ–∏—Å–∫", callback_data='search'),
         InlineKeyboardButton("üìù –ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞", callback_data='summary')],
        [InlineKeyboardButton("üìö –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π", callback_data='scientific_search'),
         InlineKeyboardButton("üåê –ü–µ—Ä–µ–≤–æ–¥", callback_data='translate')]
    ]
    return InlineKeyboardMarkup(keyboard)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    reply_markup = create_menu()
    await update.message.reply_text(
        "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –ø–æ–∏—Å–∫–∞ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
        reply_markup=reply_markup
    )

async def menu(update: Update, context: ContextTypes.DEFAULT_TYPE):
    reply_markup = create_menu()
    await update.message.reply_text(
        "–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ:",
        reply_markup=reply_markup
    )

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    
    if query.data == 'search':
        context.user_data['mode'] = 'search'
        await query.message.reply_text("–û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX), –∞ –∑–∞—Ç–µ–º –≤–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.")
    elif query.data == 'summary':
        context.user_data['mode'] = 'summary'
        await query.message.reply_text("–û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏.")
    elif query.data == 'scientific_search':
        context.user_data['mode'] = 'scientific_search_arxiv'
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞
        context.user_data['search_step'] = 'query'
        await query.message.reply_text(
            "üîç –í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.\n\n"
            "–ù–∞–ø—Ä–∏–º–µ—Ä:\n"
            "- machine learning\n"
            "- quantum computing\n"
            "- artificial intelligence\n"
            "- deep learning"
        )
    elif query.data == 'translate':
        context.user_data['mode'] = 'translate'
        await query.message.reply_text(
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π.\n"
            "–í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å:\n"
            "1. –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
            "2. –î–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX)"
        )
    elif query.data.startswith('year_range_'):
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ª–µ—Ç
        range_type = query.data.split('_')[2]
        if range_type == 'custom':
            context.user_data['search_step'] = 'year_from'
            await query.message.reply_text(
                "–í–≤–µ–¥–∏—Ç–µ –Ω–∞—á–∞–ª—å–Ω—ã–π –≥–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2020):"
            )
        else:
            current_year = datetime.now().year
            if range_type == 'last1':
                year_from = current_year - 1
                year_to = current_year
            elif range_type == 'last3':
                year_from = current_year - 3
                year_to = current_year
            elif range_type == 'last5':
                year_from = current_year - 5
                year_to = current_year
            elif range_type == 'last10':
                year_from = current_year - 10
                year_to = current_year
            else:  # all
                year_from = None
                year_to = None
                
            context.user_data['year_from'] = year_from
            context.user_data['year_to'] = year_to
            await start_search(update, context)
    elif query.data == 'start_search':
        await start_search(update, context)

async def start_search(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–∏—Å–∫ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    query = context.user_data.get('search_query')
    year_from = context.user_data.get('year_from')
    year_to = context.user_data.get('year_to')
    
    await update.callback_query.message.reply_text("üîç –ò—â—É —Å—Ç–∞—Ç—å–∏ –Ω–∞ arXiv...")
    
    try:
        articles = search(
            query=query,
            year_from=year_from,
            year_to=year_to
        )
        
        if not articles:
            await update.callback_query.message.reply_text(
                "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å—Ç–∞—Ç—å–∏ –Ω–∞ arXiv.\n"
                "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
                "1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞\n"
                "2. –°–¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º\n"
                "3. –†–∞—Å—à–∏—Ä–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω\n"
                "4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã"
            )
            return
            
        response = f"üìö –ù–∞–π–¥–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π –Ω–∞ arXiv:\n\n"
        for article in articles:
            response += format_article(article)
            
        # –†–∞–∑–±–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏
        max_length = 4096
        for i in range(0, len(response), max_length):
            await update.callback_query.message.reply_text(response[i:i + max_length])
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –Ω–∞ arXiv: {str(e)}")
        await update.callback_query.message.reply_text(
            "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å—Ç–∞—Ç–µ–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        )

def delete_file(file_path, max_retries=3, delay=1):
    for attempt in range(max_retries):
        try:
            if os.path.exists(file_path):
                os.unlink(file_path)
                logger.info(f"–í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω: {file_path}")
                return True
        except Exception as e:
            logger.warning(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} —É–¥–∞–ª–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(delay)
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫: {file_path}")
    return False

async def send_large_text(update: Update, text: str, prefix: str = "", max_chunk_size: int = 4000) -> bool:
    try:
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ–±–æ–ª—å—à–æ–π, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞—Å—Ç—è–º–∏
        if len(text) <= max_chunk_size * 10:
            chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]
            total_chunks = len(chunks)
            
            for i, chunk in enumerate(chunks, 1):
                header = f"{prefix}–ß–∞—Å—Ç—å {i} –∏–∑ {total_chunks}\n\n" if total_chunks > 1 else prefix
                try:
                    await update.message.reply_text(header + chunk)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —á–∞—Å—Ç–∏ {i}: {str(e)}")
                    try:
                        await update.message.reply_text(chunk)
                    except Exception as e:
                        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —á–∞—Å—Ç—å {i}: {str(e)}")
                        return False
            return True
        else:
            try:
                with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.txt', delete=False) as temp_file:
                    temp_file.write(text)
                    temp_file_path = temp_file.name

                with open(temp_file_path, 'rb') as file:
                    await update.message.reply_document(
                        document=file,
                        filename='translation.txt',
                        caption=f"{prefix}–ü–µ—Ä–µ–≤–æ–¥ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –≤–∏–¥–µ —Ñ–∞–π–ª–∞ –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞."
                    )
                delete_file(temp_file_path)
                return True
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
                if os.path.exists(temp_file_path):
                    delete_file(temp_file_path)
                return False
                
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
        return False

async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE):
    try:
        if 'mode' not in context.user_data:
            logger.warning("–ü–æ–ø—ã—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –±–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")
            await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã.")
            return

        file = await context.bot.get_file(update.message.document)
        file_name = update.message.document.file_name
        
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_name}, —Ä–µ–∂–∏–º: {context.user_data['mode']}")
        
        if not file_name.endswith(('.pdf', '.doc', '.docx')):
            logger.warning(f"–ü–æ–ª—É—á–µ–Ω –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_name}")
            await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF –∏–ª–∏ DOC/DOCX.")
            return

        temp_file = None
        try:
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file_name)[1])
            logger.info(f"–°–æ–∑–¥–∞–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {temp_file.name}")
            await file.download_to_drive(temp_file.name)
            temp_file.close()
            logger.info("–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω")
            logger.info("–ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            text = process_document(temp_file.name)
            delete_file(temp_file.name)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞: {str(e)}")
            logger.error(traceback.format_exc())
            if temp_file and os.path.exists(temp_file.name):
                delete_file(temp_file.name)
            raise

        if not text:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
            return

        logger.info(f"–¢–µ–∫—Å—Ç —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
        context.user_data['document_text'] = text
        
        if context.user_data['mode'] == 'search':
            logger.info("–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞: –æ–∂–∏–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞")
            await update.message.reply_text("–¢–µ–ø–µ—Ä—å –≤–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:")
        elif context.user_data['mode'] == 'translate':
            logger.info("–ù–∞—á–∞–ª–æ –ø–µ—Ä–µ–≤–æ–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            
            progress_message = await update.message.reply_text(
                "üîÑ –ü–µ—Ä–µ–≤–æ–¥ –¥–æ–∫—É–º–µ–Ω—Ç–∞...\n"
                "‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –ø–µ—Ä–µ–≤–æ–¥—É..."
            )
            
            try:
                translated_text = translate_text(text)
                if translated_text:
                    logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω. –î–ª–∏–Ω–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {len(translated_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                    await progress_message.edit_text(
                        "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω!\n"
                        "‚è≥ –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞..."
                    )

                    prefix = f"üìö –ü–µ—Ä–µ–≤–æ–¥ —Ñ–∞–π–ª–∞: {file_name}\n\n"
                    if await send_large_text(update, translated_text, prefix):
                        await progress_message.edit_text(
                            "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω!\n"
                            f"üìö –ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª: {file_name}"
                        )
                    else:
                        await progress_message.edit_text(
                            "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –ø–µ—Ä–µ–≤–æ–¥–∞.\n"
                            "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–±–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏."
                        )
                else:
                    logger.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ: –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                    await progress_message.edit_text(
                        "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n"
                        "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞ –∏–ª–∏ —Ä–∞–∑–±–µ–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏."
                    )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
                logger.error(traceback.format_exc())
                await progress_message.edit_text(
                    "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ.\n"
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ —Ä–∞–∑–±–µ–π—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –º–µ–Ω—å—à–∏–µ —á–∞—Å—Ç–∏."
                )
        else:
            logger.info("–ù–∞—á–∞–ª–æ —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏")
            await update.message.reply_text("–°–æ–∑–¥–∞—é –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É...")
            try:
                if file_name.endswith('.pdf'):
                    summary = summarize_pdf(text)
                else:
                    summary = summarize_docx(text)
                
                logger.info(f"–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ —Å–æ–∑–¥–∞–Ω–∞. –î–ª–∏–Ω–∞: {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –†–∞–∑–±–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
                max_length = 4000
                for i in range(0, len(summary), max_length):
                    chunk = summary[i:i + max_length]
                    try:
                        logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ —á–∞—Å—Ç–∏ –≤—ã–∂–∏–º–∫–∏ {i//max_length + 1}")
                        await update.message.reply_text(chunk)
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —á–∞—Å—Ç–∏ –≤—ã–∂–∏–º–∫–∏: {str(e)}")
                        shorter_chunk = chunk[:3500]
                        await update.message.reply_text(shorter_chunk)
                logger.info("–ö—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏: {str(e)}")
                logger.error(traceback.format_exc())
                await update.message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
            
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {str(e)}")
        logger.error(traceback.format_exc())
        await update.message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    if 'mode' not in context.user_data:
        await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã.")
        return

    if context.user_data['mode'] == 'translate':
        text = update.message.text
        await update.message.reply_text("üîÑ –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞...")
        
        try:
            translated_text = translate_text(text)
            if translated_text:
                # –†–∞–∑–±–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
                max_length = 4000
                for i in range(0, len(translated_text), max_length):
                    chunk = translated_text[i:i + max_length]
                    await update.message.reply_text(chunk)
            else:
                await update.message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Ç–µ–∫—Å—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            await update.message.reply_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        return

    if context.user_data['mode'] == 'scientific_search_arxiv':
        if 'search_step' not in context.user_data:
            context.user_data['search_step'] = 'query'
            
        if context.user_data['search_step'] == 'query':
            context.user_data['search_query'] = update.message.text
            
            keyboard = [
                [InlineKeyboardButton("–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥–æ–¥", callback_data='year_range_last1')],
                [InlineKeyboardButton("–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –≥–æ–¥–∞", callback_data='year_range_last3')],
                [InlineKeyboardButton("–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç", callback_data='year_range_last5')],
                [InlineKeyboardButton("–ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –ª–µ—Ç", callback_data='year_range_last10')],
                [InlineKeyboardButton("–ó–∞ –≤—Å–µ –≤—Ä–µ–º—è", callback_data='year_range_all')],
                [InlineKeyboardButton("–£–∫–∞–∑–∞—Ç—å —Å–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω", callback_data='year_range_custom')]
            ]
            await update.message.reply_text(
                "–í—ã–±–µ—Ä–∏—Ç–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞:",
                reply_markup=InlineKeyboardMarkup(keyboard)
            )
            
        elif context.user_data['search_step'] == 'year_from':
            try:
                year_from = int(update.message.text)
                if year_from < 1991 or year_from > datetime.now().year:
                    await update.message.reply_text(
                        f"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≥–æ–¥ –º–µ–∂–¥—É 1991 –∏ {datetime.now().year}:"
                    )
                    return
                    
                context.user_data['year_from'] = year_from
                context.user_data['search_step'] = 'year_to'
                await update.message.reply_text(
                    f"–í–≤–µ–¥–∏—Ç–µ –∫–æ–Ω–µ—á–Ω—ã–π –≥–æ–¥ (–æ—Ç {year_from} –¥–æ {datetime.now().year}):"
                )
            except ValueError:
                await update.message.reply_text(
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≥–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2020):"
                )
                
        elif context.user_data['search_step'] == 'year_to':
            try:
                year_to = int(update.message.text)
                year_from = context.user_data['year_from']
                
                if year_to < year_from or year_to > datetime.now().year:
                    await update.message.reply_text(
                        f"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≥–æ–¥ –º–µ–∂–¥—É {year_from} –∏ {datetime.now().year}:"
                    )
                    return
                    
                context.user_data['year_to'] = year_to
            
                keyboard = [[InlineKeyboardButton("üîç –ù–∞—á–∞—Ç—å –ø–æ–∏—Å–∫", callback_data='start_search')]]
                await update.message.reply_text(
                    f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞:\n"
                    f"üìù –ó–∞–ø—Ä–æ—Å: {context.user_data['search_query']}\n"
                    f"üìÖ –ì–æ–¥—ã: {year_from} - {year_to}\n\n"
                    f"–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –¥–ª—è –Ω–∞—á–∞–ª–∞ –ø–æ–∏—Å–∫–∞:",
                    reply_markup=InlineKeyboardMarkup(keyboard)
                )
            except ValueError:
                await update.message.reply_text(
                    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≥–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 2023):"
                )
    elif context.user_data['mode'] == 'search':
        if 'document_text' not in context.user_data:
            await update.message.reply_text("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç.")
            return
            
        query = update.message.text
        text = context.user_data['document_text']
        
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        embeddings = model.encode(sentences, convert_to_tensor=True)
        query_embedding = model.encode(query, convert_to_tensor=True)
        
        cos_scores = util.cos_sim(query_embedding, embeddings)[0]
        top_results = torch.topk(cos_scores, k=min(5, len(sentences)))
        
        response = "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:\n\n"
        for score, idx in zip(top_results[0], top_results[1]):
            if score > 0.3:
                response += f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.2f}\n"
                response += f"–¢–µ–∫—Å—Ç: {sentences[idx]}\n"
                response += "---\n"
        
        await update.message.reply_text(response)

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /help"""
    help_text = (
        "ü§ñ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:\n\n"
        "/start - –ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º\n"
        "/menu - –ü–æ–∫–∞–∑–∞—Ç—å –º–µ–Ω—é –¥–µ–π—Å—Ç–≤–∏–π\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
        "/search - –ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º\n"
        "/summary - –°–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É\n"
        "/scientific - –ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π\n"
        "/translate - –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞\n\n"
        "üìù –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n"
        "1. –í—ã–±–µ—Ä–∏—Ç–µ –Ω—É–∂–Ω—É—é –∫–æ–º–∞–Ω–¥—É –∏–∑ –º–µ–Ω—é\n"
        "2. –°–ª–µ–¥—É–π—Ç–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º –±–æ—Ç–∞\n"
        "3. –î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ /menu"
    )
    await update.message.reply_text(help_text)

async def search_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data['mode'] = 'search'
    await update.message.reply_text("–û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX), –∞ –∑–∞—Ç–µ–º –≤–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.")

async def summary_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data['mode'] = 'summary'
    await update.message.reply_text("–û—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫—Ä–∞—Ç–∫–æ–π –≤—ã–∂–∏–º–∫–∏.")

async def scientific_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data['mode'] = 'scientific_search_arxiv'
    await update.message.reply_text(
        "üîç –í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –≤ —Ñ–æ—Ä–º–∞—Ç–µ:\n"
        "–∑–∞–ø—Ä–æ—Å | –≥–æ–¥_–æ—Ç | –≥–æ–¥_–¥–æ\n\n"
        "–ù–∞–ø—Ä–∏–º–µ—Ä:\n"
        "machine learning | 2020 | 2023\n"
        "quantum computing | 2019 | 2023\n"
        "artificial intelligence | 2018 | 2023\n\n"
        "–ï—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –≥–æ–¥—ã, –ø—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
    )

async def translate_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    context.user_data['mode'] = 'translate'
    await update.message.reply_text(
        "–û—Ç–ø—Ä–∞–≤—å—Ç–µ —Ç–µ–∫—Å—Ç –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–π.\n"
        "–í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å:\n"
        "1. –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\n"
        "2. –î–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOC/DOCX)"
    )

async def setup_commands(application: Application):
    commands = [
        BotCommand("start", "–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å –±–æ—Ç–æ–º"),
        BotCommand("menu", "–ü–æ–∫–∞–∑–∞—Ç—å –º–µ–Ω—é –¥–µ–π—Å—Ç–≤–∏–π"),
        BotCommand("help", "–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∫–æ–º–∞–Ω–¥"),
        BotCommand("search", "–ü–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º"),
        BotCommand("summary", "–°–æ–∑–¥–∞—Ç—å –∫—Ä–∞—Ç–∫—É—é –≤—ã–∂–∏–º–∫—É"),
        BotCommand("scientific", "–ü–æ–∏—Å–∫ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π"),
        BotCommand("translate", "–ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞")
    ]
    await application.bot.set_my_commands(commands)

def translate_text(text, target_lang='ru'):
    try:
        logger.info("–ù–∞—á–∞–ª–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–µ—Ä–µ–≤–æ–¥–∞")
        if not yandexAPI:
            logger.error("API –∫–ª—é—á –Ø–Ω–¥–µ–∫—Å –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return None
            
        if not folder_id:
            logger.error("ID –ø–∞–ø–∫–∏ –Ø–Ω–¥–µ–∫—Å –û–±–ª–∞–∫–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return None

        chr_reqest = 9500  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
        MAX_REQUESTS_PER_MINUTE = 20   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–∏–Ω—É—Ç—É
        delay = 3.0                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        logger.info("–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        sentences = sent_tokenize(text)
        logger.info(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
        parts = []
        current_part = []
        current_length = 0
        
        for sentence in sentences:
            sentence_length = len(sentence)
            if sentence_length > chr_reqest:
                if current_part:
                    parts.append(' '.join(current_part))
                    current_part = []
                    current_length = 0

                words = sentence.split()
                current_words = []
                current_words_length = 0

                for word in words:
                    word_length = len(word) + 1  # +1 –¥–ª—è –ø—Ä–æ–±–µ–ª–∞
                    if current_words_length + word_length > chr_reqest:
                        if current_words:
                            parts.append(' '.join(current_words))
                        current_words = [word]
                        current_words_length = word_length
                    else:
                        current_words.append(word)
                        current_words_length += word_length
                
                if current_words:
                    parts.append(' '.join(current_words))
                continue
            
            if current_length + sentence_length > chr_reqest:
                if current_part:
                    parts.append(' '.join(current_part))
                current_part = [sentence]
                current_length = sentence_length
            else:
                current_part.append(sentence)
                current_length += sentence_length
        
        if current_part:
            parts.append(' '.join(current_part))
            
        logger.info(f"–¢–µ–∫—Å—Ç —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(parts)} —á–∞—Å—Ç–µ–π –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞")
        
        translated_parts = []
        total_parts = len(parts)
        
        # API –Ø–Ω–¥–µ–∫—Å –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞
        url = "https://translate.api.cloud.yandex.net/translate/v2/translate"
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Api-Key {yandexAPI}"
        }
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —á–∞—Å—Ç—å
        for i, part in enumerate(parts, 1):
            logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ —á–∞—Å—Ç–∏ {i}/{total_parts} (–¥–ª–∏–Ω–∞: {len(part)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            if len(part) > chr_reqest:
                logger.warning(f"–ß–∞—Å—Ç—å {i} –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤ ({len(part)} > {chr_reqest})")
                # –†–∞–∑–±–∏–≤–∞–µ–º —á–∞—Å—Ç—å –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ —á–∞—Å—Ç–∏
                sub_parts = [part[j:j + chr_reqest] for j in range(0, len(part), chr_reqest)]
                for j, sub_part in enumerate(sub_parts, 1):
                    logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ –ø–æ–¥—á–∞—Å—Ç–∏ {j}/{len(sub_parts)} —á–∞—Å—Ç–∏ {i}")
                    try:
                        body = {
                            "targetLanguageCode": target_lang,
                            "texts": [sub_part],
                            "folderId": folder_id
                        }
                        
                        response = requests.post(url, headers=headers, json=body, timeout=30)
                        if response.status_code == 200:
                            translation = response.json()
                            if 'translations' in translation and translation['translations']:
                                translated_parts.append(translation['translations'][0]['text'])
                                logger.info(f"–ü–æ–¥—á–∞—Å—Ç—å {j} —á–∞—Å—Ç–∏ {i} —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞")
                            else:
                                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ API –¥–ª—è –ø–æ–¥—á–∞—Å—Ç–∏ {j} —á–∞—Å—Ç–∏ {i}")
                        else:
                            logger.error(f"–û—à–∏–±–∫–∞ API –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –ø–æ–¥—á–∞—Å—Ç–∏ {j} —á–∞—Å—Ç–∏ {i}: {response.text}")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ –ø–æ–¥—á–∞—Å—Ç–∏ {j} —á–∞—Å—Ç–∏ {i}: {str(e)}")
                        continue
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    if j < len(sub_parts):
                        time.sleep(delay)
                continue
            
            body = {
                "targetLanguageCode": target_lang,
                "texts": [part],
                "folderId": folder_id
            }
            
            try:
                logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ API –¥–ª—è —á–∞—Å—Ç–∏ {i}")
                response = requests.post(url, headers=headers, json=body, timeout=30)
                
                if response.status_code != 200:
                    logger.error(f"–û—à–∏–±–∫–∞ API (—Å—Ç–∞—Ç—É—Å {response.status_code}): {response.text}")
                    if response.status_code == 401:
                        logger.error("–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –≤ –Ø–Ω–¥–µ–∫—Å API. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á")
                        return None
                    elif response.status_code == 403:
                        logger.error("–î–æ—Å—Ç—É–ø –∑–∞–ø—Ä–µ—â–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞ –∏ ID –ø–∞–ø–∫–∏")
                        return None
                    elif response.status_code == 429:
                        logger.error("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API")
                        wait_time = 60 
                        logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ {wait_time} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ API: {response.text}")
                        return None
                
                translation = response.json()
                logger.debug(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç API –¥–ª—è —á–∞—Å—Ç–∏ {i}")
                
                if 'translations' in translation and translation['translations']:
                    translated_text = translation['translations'][0]['text']
                    translated_parts.append(translated_text)
                    logger.info(f"–ß–∞—Å—Ç—å {i} —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–∞")
                else:
                    logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ API –¥–ª—è —á–∞—Å—Ç–∏ {i}: {translation}")
                    continue
                    
            except requests.exceptions.Timeout:
                logger.error(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —á–∞—Å—Ç–∏ {i}")
                time.sleep(delay * 2)
                continue
            except requests.exceptions.RequestException as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —á–∞—Å—Ç–∏ {i}: {str(e)}")
                time.sleep(delay)
                continue
            except json.JSONDecodeError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–±–æ—Ä–∞ JSON –¥–ª—è —á–∞—Å—Ç–∏ {i}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —á–∞—Å—Ç–∏ {i}: {str(e)}")
                logger.error(traceback.format_exc())
                continue
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –¥–ª—è –ª–∏–º–∏—Ç–æ–≤ API
            if i < total_parts:
                logger.debug(f"–û–∂–∏–¥–∞–Ω–∏–µ {delay} —Å–µ–∫—É–Ω–¥ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –∑–∞–ø—Ä–æ—Å–æ–º")
                time.sleep(delay)
        
        if not translated_parts:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∏ –æ–¥–Ω–æ–π —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞")
            return None
            
        logger.info(f"–ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω. –£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω–æ {len(translated_parts)} –∏–∑ {total_parts} —á–∞—Å—Ç–µ–π")
        return ' '.join(translated_parts)
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Ç–µ–∫—Å—Ç–∞: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def main():
    try:
        if not yandexAPI :
            logger.warning("API –∫–ª—é—á –Ø–Ω–¥–µ–∫—Å –ü–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –§—É–Ω–∫—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
            
        application = Application.builder().token(telegramAPI).post_init(setup_commands).build()
        application.add_handler(CommandHandler("start", start))
        application.add_handler(CommandHandler("menu", menu))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("search", search_command))
        application.add_handler(CommandHandler("summary", summary_command))
        application.add_handler(CommandHandler("scientific", scientific_command))
        application.add_handler(CommandHandler("translate", translate_command))
        application.add_handler(CallbackQueryHandler(button_handler))
        application.add_handler(MessageHandler(filters.Document.ALL, handle_document))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))

        logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
        application.run_polling(allowed_updates=Update.ALL_TYPES)
    except telegram.error.Conflict as e:
        logger.error("–û–±–Ω–∞—Ä—É–∂–µ–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç: –¥—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–æ—Ç–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
        print("–û—à–∏–±–∫–∞: –î—Ä—É–≥–æ–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–æ—Ç–∞ —É–∂–µ –∑–∞–ø—É—â–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Å–µ –¥—Ä—É–≥–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –±–æ—Ç–∞ –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
    except Exception as e:
        logger.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")
        print(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")

if __name__ == '__main__':
    main() 